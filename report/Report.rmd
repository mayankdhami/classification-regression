---
title: "Machine Learning Tasks - Classification and Regression"
author: "Mayank Dhami"
date: "11/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## 1. Description of Classification Dataset

The dataset contains instances from a research that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.
There were a total of 306 instances recorded, with a pool of four variables namely the following:

1. Age of patient at time of operation (numerical)
2. Patient's year of operation (year - 1900, numerical)
3. Number of positive axillary nodes detected (numerical)
4. Survival status (class attribute):
          1 = the patient survived 5 years or longer,
          2 = the patient died within 5 year
     
The task is to correctly predict whether the patient lived for more than 5 years of 
operation or not. 
Before creating a model, preliminary analysis of the data makes sense.

The distribution of patients who died and survived after 5 years of operation goes as follows:  

```{r message = FALSE, warning = FALSE, include=FALSE}
library(ggplot2)
library(class)
library(caret)
library(gmodels)
library(dplyr)
survivalknn <- read.table("../data/haberman1.txt",sep = ",")
# Inspect the data
summary(survivalknn)
str(survivalknn)
#Giving apt variable names, labels to factors and making year field easy to interpret 
survivalknn = survivalknn %>% rename(Age = V1,Year_of_Operation = V2,
                                     No_of_positive_auxillary_nodes_detected = V3,
                                     Survival_status = V4)
survivalknn$Survival_status = factor(survivalknn$Survival_status,
                                     levels = c(1,2),
                                     labels = c("Survived>5yr","Died<5yr"))
survivalknn$Year_of_Operation = survivalknn$Year_of_Operation+1900 
str(survivalknn)

#Proportion that should be maintained in samples as well
round(prop.table(table(survivalknn$Survival_status))*100, 1)

survstat = ggplot(survivalknn, aes(x= Survival_status,fill = Survival_status)) + geom_bar(width = 0.75)+
  geom_text(stat='count', aes(label= ..count..), vjust=3)+
  scale_fill_manual(values=c("#55DDE0", "#33658A"))+
  labs(x = "", y = "", fill = NULL, title = expression(bold("Survival count of people")))+
  theme_classic() + theme(axis.line = element_blank(),
                          axis.text = element_blank(),
                          axis.ticks = element_blank(),
                          plot.title = element_text(hjust = 0.5, color = "#666666"))
#Analysing different variables and relationships
agesurv = ggplot(survivalknn, aes(x=Survival_status, y=Age, fill=Survival_status)) +
  geom_boxplot(width = 0.5)+
  stat_summary(fun=mean, geom="point", shape=20, size=8, color="black", fill="red") +
  scale_fill_manual(values=c("#999999", "#E69F00"))+
  labs(x = "Survival Status", y = "Age", fill = NULL, title = expression(bold("Survival status by age")))+
  theme_classic() + theme(plot.title = element_text(hjust = 0.5, color = "#666666"))
#As mean and median age of people with the disease, who survived and died within
#5 years were similar, we will check for their range.
with(survivalknn,by(Age,Survival_status,max))
with(survivalknn,by(Age,Survival_status,min))

#year of operation and SUrvival status
yearsurv = ggplot(survivalknn, aes(x=Survival_status, y=Year_of_Operation, fill=Survival_status)) +
  geom_boxplot(width = 0.5)+
  stat_summary(fun=mean, geom="point", shape=20, size=8, color="black", fill="red") +
  scale_fill_manual(values=c("#999999", "#E69F00"))+
  labs(x = "Survival Status", y = "Year of Operation", fill = NULL, title = expression(bold("Survival status by year of operation")))+
  theme_classic() + theme(plot.title = element_text(hjust = 0.5, color = "#666666"))
table(survivalknn$Survival_status,survivalknn$Year_of_Operation)

#Year of operation and no. of positive auxilliary nodes
yearnodes = ggplot(survivalknn, aes(x=Year_of_Operation,y= No_of_positive_auxillary_nodes_detected)) +
    geom_point(position=position_jitter(w=0.1,h=0)) +
    xlab('Year of Operation')+ylab('No of positive auxillary nodes detected')
#No correlation also
with(survivalknn, cor(Year_of_Operation,No_of_positive_auxillary_nodes_detected))

#Age and no. of positive auxilliary nodes
agenodes = ggplot(survivalknn, aes(x=Age,y= No_of_positive_auxillary_nodes_detected)) +
  geom_point(position=position_jitter(w=0.1,h=0)) +
  xlab('Age at operation')+ylab('No of positive auxillary nodes detected')
#No correlation also
with(survivalknn, cor(Age,No_of_positive_auxillary_nodes_detected))

##Data partitioning and training a model on data 
#Z-score scaling since year value is too large and it will affect results
survival_knn = as.data.frame(scale(survivalknn[-4]))
#Making training and test data
set.seed(123)
surv_train <- survival_knn[1:245, ]
summary(surv_train)
surv_test <- survival_knn[246:306, ]
summary(surv_test)
survival_train_labels = survivalknn[1:245,4]
survival_test_labels = survivalknn[246:306,4]
surv_test_pred = knn(surv_train, surv_test, survival_train_labels, 1)
#Evaluating results 
CrossTable(survival_test_labels, surv_test_pred, prop.chisq = F)
#Since 15 instances were incorrectly identified, out of a total of 61.
confusionMatrix(surv_test_pred,survival_test_labels)
accur1 = (1-(16/61))*100
accur1
tp = 39
fn = 6
fp = 10
tn = 6
sens = tp/(tp+fn)
spec = tn/(fp+tn)
prec = tp/(tp+fp)
falspos = 1- spec
metrics = data.frame(Accuracy = accur1, Sensitivity = sens*100,Specificity = spec*100,Precision = prec*100,False_Positive_Rate = falspos*100)


###Decision Tree Method
library(caret)
library(rpart)
library(e1071)
survival <- survivalknn
summary(survival)
str(survival)
#Split the data into training and test set (80/20 split)
set.seed(123)
survival.training <- survival$Survival_status %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.set <- survival[survival.training, ]
summary(train.set)
test.set <- survival[-survival.training, ]
summary(test.set)
# Training the model on dataset
set.seed(123)
model1 <- rpart(Survival_status ~., 
                data = train.set, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
# Make predictions on the test data
predicted.class <- model1 %>%
  predict(test.set, type = "class")
head(predicted.class)
mean(predicted.class == test.set$Survival_status)
confusionMatrix(predicted.class,test.set$Survival_status)
### Pruning the tree to find alternative models
# Fit the model on the training set
set.seed(123)
model2 <- train(
  Survival_status ~., data = train.set, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
# Plot model accuracy vs different values of
# cp (complexity parameter)
plot(model2)
# Print the best tuning parameter cp that
# maximizes the model accuracy
model2$bestTune
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model2$finalModel)
text(model2$finalModel, digits = 3)
model2$finalModel
# Make predictions on the test data
predicted.classes <- model2 %>% predict(test.set)
# Compute model accuracy rate on test data
mean(predicted.classes == test.set$Survival_status)
confusionMatrix(predicted.classes,test.set$Survival_status)
```

```{r echo=FALSE}
survstat
```


The operations had a success rate of around 73.5%(225) where patients survived longer than
five years and the remaining 26.5%(81) died before the tenure of five years. 


Relationships between certain variables need to be investigated, for a better understanding
of the dataset.

```{r echo=FALSE}
agesurv
```

Th mean and median age of people with the disease, in the two categories were similar as visible from the graph above, so we will check for their range with the maximum and minimum values in both categories respectively.

```{r echo=FALSE}
with(survivalknn,by(Age,Survival_status,max))
with(survivalknn,by(Age,Survival_status,min))
```

The minimum(30) and maximum(77) age in case of survivors was lesser as compared to the people who could not make it through 5 years after operation who had a minimum of 34 years and a maximum of 83 years.

```{r echo=FALSE}
yearsurv
```

The above graph shows that the mean and median Year of Operation for both the survival 
categories was similar, and so were the maximum and minimum values.

The only fact separating these two categories seems to be that there were more entries
with year of operation less than 1963 for the people who died before 5 year tenure.

Also, there seems to be no obvious relationship between Year of operation and No.of positive auxillary nodes found, and Age of patient at the time of operation and No.of positive auxillary nodes found which can be seen in the following graphs.

```{r echo=FALSE}
yearnodes
agenodes
```

## 2. Description of Regression Dataset

The regression dataset is the market historical data set of real estate valuation, collected from Sindian Dist., New Taipei City, Taiwan. There are 414 instances of data over seven variables including the following:

X1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)

X2=the house age (unit: year)

X3=the distance to the nearest Mass Rapid Transit(MRT) station (unit: meter)

X4=the number of convenience stores in the living circle on foot (integer)

X5=the geographic coordinate, latitude. (unit: degree)

X6=the geographic coordinate, longitude. (unit: degree)

Y= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter^2)

The variable originally marked as Y was the dependent variable to be predicted.
```{r message = FALSE, warning = FALSE, include=FALSE}
library(corrplot)
library(psych)
library(xlsx)
library(MLmetrics)
realestate = read.xlsx("../data/real_estate.xlsx",1)
#Dropping unnecessary ID column
realestate = realestate[-c(271,313,114),-1]
#Renaming variables to something more apt
realestate = realestate %>% rename(Transaction_date = names(realestate[1]),House_Age = names(realestate[2]),
                                   Distance_to_nearest_MRT_station = names(realestate[3]),
                                   No_of_convenience_stores = names(realestate[4]),
                                   Latitude = names(realestate[5]),
                                   Longitude = names(realestate[6]),
                                   House_price_of_unit_area = names(realestate[7]))
summary(realestate)
str(realestate)
#Dependent variable normality check - looks normally distributed 
layout(1)
qqnorm(realestate$House_price_of_unit_area)
hist(realestate$House_price_of_unit_area)
#Correlations among variables
cor(realestate[1:7])
corrplot(cor(realestate[1:7]),"square","full")
pairs.panels(realestate[1:7])
#Basic Model with all the variables, partitioning into test and train
set.seed(123)
realestate.train <- realestate$House_price_of_unit_area %>% 
  createDataPartition(p = 0.8, list = FALSE)
estate.train = realestate[realestate.train,]
summary(estate.train)
estate.test = realestate[-realestate.train,]
property_model = lm(House_price_of_unit_area ~ ., estate.train)
#Summarizing model stats
summary(property_model)
layout(1)
layout(matrix(1:4,2,2))
#Checking residual plots of model
plot(property_model)
#Checking for Mean Absolute Error Percentage to compare with improved model
predicted.price = as.data.frame(predict(property_model, estate.test[,-7]))
names(predicted.price)[1] = "pred_house_vals"
M1_err = MAPE(predicted.price$pred_house_vals,estate.test$House_price_of_unit_area)*100

##Improving basic model with non linear relationships and interaction effects
#Making a copy of original dataset
realestatev2 = realestate  
realestatev2$House_Age2 <- realestatev2$House_Age^2
set.seed(123)
realestate.trainv2 <- realestatev2$House_price_of_unit_area %>% 
  createDataPartition(p = 0.8, list = FALSE)
estate.trainv2 = realestatev2[realestate.trainv2,]
summary(estate.trainv2)
estate.testv2 = realestatev2[-realestate.trainv2,]
property_modelv2 = lm(House_price_of_unit_area ~ Transaction_date+House_Age+House_Age2+Distance_to_nearest_MRT_station
                      *No_of_convenience_stores+Latitude+Longitude*Distance_to_nearest_MRT_station, estate.trainv2)
summary(property_modelv2)
predicted.pricev2 = as.data.frame(predict(property_modelv2, estate.testv2[,-7]))
names(predicted.pricev2)[1] = "pred_house_vals"
M2_err = MAPE(predicted.pricev2$pred_house_vals,estate.testv2$House_price_of_unit_area)*100
#Difference in Error Rates  
Diff = M1_err - M2_err 
```

First of all, before analysis, whether the dependent variable is normally distributed or not
was made sure of using the following qqplots and histogram of the variable, which indicate
it is normally distributed indeed.

```{r echo=FALSE}
qqnorm(realestate$House_price_of_unit_area)
hist(realestate$House_price_of_unit_area,xlab = "House Price of Unit Area",
     main = "Histogram of House Price of Unit Area")
```

Looking for further relationships among variables by checking for correlations as follows:

```{r echo=FALSE}
corrplot(cor(realestate[1:7]),"square","full")
pairs.panels(realestate[1:7])
```

We can see some variables highly correlating with house price of unit area such as distance
to mass rapid transport, no. of convenience stores, latitude and longitude, which make 
sense normally.

## 3. Classification Analysis
### a) Using Decision Trees

Training and testing datasets were a split of 80-20 made using caret package, and the initial proportion of both the categories of interest was maintained in these as well. 
The preliminary model trained using decision tree using rpart package gave the following tree with the following evaluation metrics: 

```{r echo=FALSE}
par(xpd = NA)
plot(model1)
text(model1, digits = 3)
confusionMatrix(predicted.class,test.set$Survival_status)
```

Although we got a more than decent accuracy of 77%, we would like the false positives(11)
to decrease in number if possible, so we can more accurately predict who lived for more than
five years. We tried pruning the tree to look for a less complex tree to solve any overfitting issues that might be happening now. 

The pruned tree with its evaluation metrics is given below:
```{r echo=FALSE}
par(xpd = NA) # Avoid clipping the text in some device
plot(model2$finalModel)
text(model2$finalModel, digits = 3)
confusionMatrix(predicted.classes,test.set$Survival_status)
```

Since the number of false positives decreased from 11 to 7, the pruned tree has increased precision(positive pred value) from 0.792 to 0.841 and also reduced false positive rate(1-Specificity) from 0.687 to 0.437, and the sensitivity and specificity also look
much more balanced now, at the cost of very slight drop in accuracy to 75.4%. The tree looks much more interpretable now as well. The pruned tree used 10 turns of cross validation with the best complexity parameter(cp) value out of 10, to make this final model.

### b) Using kNN

Training and testing datasets were a split of 80-20, and the initial proportion of both the categories of interest was maintained in these as well. Before partitioning, Z-scaling of data was done to make the impact of excessively large valued Year of Operation variable similar to others.

Then, a value for k as 1, was chosen after checking for most favourable outcome in terms of good accuracy and less false positive rates among other values.
The results of evaluation of model using kNN are as follows:

```{r echo=FALSE}
CrossTable(survival_test_labels, surv_test_pred, prop.chisq = F)
metrics
```

The accuracy we got with kNN was almost same as improved decision tree method, but with a proper value of k, we also managed to balance sensitivity and specificity, along with increasing precision and decreasing false positive rate as compared to other k values. 

## 4. Regression Analysis

After partitioning the dataset into training and test(80-20) and checking for data aptness for linear regression, a preliminary model with all the variables except the dependent variable(House Price), was trained whose statistics can be seen below:

Adjusted R-squared value of 0.65 says that this amount of variation(65%) in dependent variable can be attributed to the model. Except longitude, every variable looks significant enough to the model to make an impact. A small value of residual error and a F-statistic >> 1 suggest that this model is doing somewhat of a good job.

```{r echo=FALSE}
summary(property_model)
```

To improve the model, we added non linear relationships using House_Age variable and interaction effects using variables: Distance_to_nearest_MRT_station and         No_of_convenience_stores & Longitude and Distance_to_nearest_MRT_station.
We got an improved model with the following statistics:

Adjusted R-squared value increased to 0.748, which means we are better able to explain dependent variable using our new model. Longitude itself was of not much significance, but with Distance to nearest MRT, it made a lot of sense. Lower maximum and minimum values of residuals are also a good sign that our predictions are not much off. 

```{r echo=FALSE}
summary(property_modelv2)
plot(property_modelv2)
```

Mean Absolute Percentage Error(MAPE) has been calculated to evaluate the predictions of the two models compared to real values and get another source to support the new model. The MAPE for both the models are as below, where we can see there is a difference of 3% in the error rates in favour of new model. 

```{r echo=FALSE}
print(paste("Model 1: ",round(M1_err,2)))
print(paste("Model 2: ",round(M2_err,2)))
print(round(Diff,2))
```

## 5. Reflection 

Classification and Regression are the two most common machine learning tasks, which were attempted here to gain a better understanding of how their algorithms work from behind. A cancer survival set was used for classification, where the decision tree method proved out to be doing slightly better than kNN. It does generalize better to most problems which is a fact well known. 

Although decision tree needed some pruning using best tuned cp, it gave a far less complex tree than the un-pruned model with much more balanced evaluation metrics, specifically high precision and low false positive rate by reducing the number of misclassified cases of false positives, which is when a patient was said to have survived but in actual, they did not.  

Multiple Linear Regression task was carried on a real estate dataset where we saw a model on all the independent variables being overtaken by a model built by adding interaction effects among variables and non-linear relationships. There were certain variables which made sense to be more significant such as age of the house, no. of convenience stores nearby, distance to nearest MRT stations than the others in determining the output variable, house price of unit area. As the house age increases, the house price of unit area may become disproportionately cheaper for the oldest houses. Longitude was not having much effect on its own, but when combined with Distance to nearest MRT station, it became highly significant. 

The older model had a lesser R-squared value along with slightly more residual error and a greater range of intercept value than the newer model. Z-score scaling was found to be working better in this case as compared to basic normalization of variables using maximum and minimum values.  

### Challenges faced

a.	kNN gave a host of different results which came with varying values of k, which had to extensively checked for each value, as each value of k also gave different values in Confusion Matrix. The task to run upon the best option through hit and trial was a little time consuming.

b.	When the updated models are built, it was hard to base a methodology to accept a newer model with slightly less accuracy than older model in decision trees, but the importance of other evaluation metrics was helpful in determining the better choice.

c.	Residual charts in regression helped finding 4 instances which were reducing the R-squared value by 0.10 all by themselves, but these instances had to be removed after thinking for some time, whether it would be apt or not. 

d.	An evaluation metric was missing for predicted values in regression to choose the better model for sure and to make sense of results obtained, which was found in MAPE, after having researched about it for sometime.
